{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOeq7FYZX7nNfS+jf7VBhv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nahom32/ViT/blob/main/notebooks/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f5ELwdeVG7_N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = datasets.CIFAR10(root='./data/cifar-10', train=True, download=True, transform=transform)\n",
        "test_data = datasets.CIFAR10(root='./data/cifar-10', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfh65h_arVOt",
        "outputId": "3f961a19-9539-4427-d315-c7a41ad6d897"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 47.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size = 32, patch_size = 4, in_chans = 3, embed_dim = 768):\n",
        "        super().__init__()\n",
        "        self.img_size   = img_size\n",
        "        self.patch_size = patch_size    # P\n",
        "        self.in_chans   = in_chans      # C\n",
        "        self.embed_dim  = embed_dim     # D\n",
        "\n",
        "        self.num_patches = (img_size // patch_size) ** 2        # N = H*W/P^2\n",
        "        self.flatten_dim = patch_size * patch_size * in_chans   # P^2*C\n",
        "\n",
        "        self.proj = nn.Linear(self.flatten_dim, embed_dim) # (P^2*C,D)\n",
        "\n",
        "        self.position_embed = nn.Parameter(torch.zeros(1, 1 + self.num_patches, embed_dim))\n",
        "        self.class_embed    = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.reshape(1, -1, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 1, 3).reshape(B, self.num_patches, -1)\n",
        "\n",
        "        x = self.proj(x)\n",
        "\n",
        "        cls_emb = self.class_embed.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_emb, x), dim = 1)\n",
        "\n",
        "        x = x + self.position_embed\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ZsNTr5pPy6KH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_embed = PatchEmbedding()\n",
        "\n",
        "embeddings = patch_embed(torch.stack([train_data[i][0] for i in range(10)]))\n",
        "print(embeddings.shape)\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19tRKjhCavJo",
        "outputId": "42f1c8d8-eaa5-4c14-f415-c9dcd7cd2419"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 65, 768])\n",
            "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [-5.2078e-01,  1.0504e-02,  5.4104e-01,  ...,  1.8488e-01,\n",
            "          -3.0784e-01,  1.0644e-01],\n",
            "         [-4.0490e-01,  4.1700e-03,  4.2748e-01,  ...,  1.1319e-01,\n",
            "          -2.0800e-01, -2.2263e-02],\n",
            "         ...,\n",
            "         [-3.0173e-01,  7.5302e-02,  3.5745e-01,  ...,  1.8374e-01,\n",
            "          -1.4220e-01,  1.4374e-01],\n",
            "         [-3.3998e-01, -3.5417e-02,  3.2596e-01,  ...,  1.0464e-01,\n",
            "          -6.7990e-02, -7.4361e-02],\n",
            "         [-1.7090e-01, -1.3885e-02,  2.8166e-01,  ...,  1.0261e-01,\n",
            "          -6.6580e-02, -3.1313e-02]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [-6.6811e-01, -1.0550e-01,  5.6385e-01,  ..., -4.7664e-03,\n",
            "          -2.9877e-01, -7.1115e-02],\n",
            "         [-3.9925e-01, -3.2990e-03,  3.9107e-01,  ...,  1.3110e-01,\n",
            "          -9.6710e-02, -8.2381e-03],\n",
            "         ...,\n",
            "         [-4.1846e-01, -2.7282e-01,  3.4286e-01,  ..., -1.0820e-01,\n",
            "          -2.9556e-01, -9.6719e-02],\n",
            "         [-4.7741e-01, -8.6843e-02,  4.5292e-01,  ...,  1.6361e-02,\n",
            "          -7.6790e-02, -9.3113e-02],\n",
            "         [-4.5228e-01, -1.0775e-01,  4.6332e-01,  ...,  2.6719e-02,\n",
            "          -7.9874e-02, -8.3537e-02]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [-6.2960e-01, -1.8350e-02,  4.9477e-01,  ...,  1.7183e-01,\n",
            "          -2.2984e-01,  3.6618e-02],\n",
            "         [-4.9769e-01,  8.3596e-02,  3.5794e-01,  ...,  1.7675e-01,\n",
            "          -1.8620e-01,  7.6353e-02],\n",
            "         ...,\n",
            "         [-8.0527e-01,  3.0836e-02,  6.8575e-01,  ...,  1.1106e-01,\n",
            "          -5.5097e-01, -5.3779e-02],\n",
            "         [-7.1500e-01, -2.2697e-02,  6.3969e-01,  ...,  6.9326e-02,\n",
            "          -1.7885e-01,  9.0283e-02],\n",
            "         [-1.8021e-01, -2.3517e-02,  4.1871e-01,  ..., -3.4318e-02,\n",
            "          -2.6063e-01,  2.1848e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [-6.0453e-01, -2.0827e-02,  4.5594e-01,  ...,  1.4933e-01,\n",
            "          -2.4374e-01,  1.4848e-02],\n",
            "         [-3.6239e-01, -4.3149e-02,  3.7511e-01,  ...,  7.3695e-02,\n",
            "          -1.0285e-01, -3.7264e-02],\n",
            "         ...,\n",
            "         [-6.7887e-01,  3.5600e-02,  4.5111e-01,  ...,  2.1520e-01,\n",
            "          -2.2597e-01, -1.0098e-01],\n",
            "         [-3.5137e-01,  1.9126e-02,  4.4051e-01,  ...,  2.0509e-01,\n",
            "          -1.7130e-01,  1.8467e-01],\n",
            "         [-2.8496e-01,  7.9452e-02,  2.2817e-01,  ...,  1.3716e-01,\n",
            "          -2.3608e-01, -4.4877e-02]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [-1.3049e+00, -2.7015e-03,  8.1997e-01,  ...,  3.3090e-01,\n",
            "          -3.9277e-01,  9.5528e-02],\n",
            "         [-9.2779e-01,  8.5165e-02,  7.4456e-01,  ...,  6.7573e-02,\n",
            "          -4.6136e-01,  2.6217e-01],\n",
            "         ...,\n",
            "         [-1.8176e-01, -1.0557e-01,  1.8239e-01,  ...,  1.0775e-01,\n",
            "           3.8418e-02, -9.8454e-02],\n",
            "         [-5.3045e-02, -9.0559e-02,  3.1131e-01,  ..., -7.8703e-02,\n",
            "           2.5912e-03, -4.6122e-02],\n",
            "         [-2.9495e-02, -7.3585e-02,  1.9823e-01,  ..., -2.2338e-02,\n",
            "          -6.1232e-02, -7.8706e-02]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [-5.7082e-01, -3.7294e-02,  3.9502e-01,  ...,  1.0922e-01,\n",
            "          -2.7637e-01, -2.8975e-02],\n",
            "         [-7.2413e-01, -1.3345e-02,  2.8653e-01,  ...,  2.4613e-01,\n",
            "          -2.5191e-01,  6.6169e-02],\n",
            "         ...,\n",
            "         [-2.5227e-03, -9.7683e-02,  2.2603e-01,  ...,  4.3342e-02,\n",
            "          -3.2657e-02, -1.4711e-02],\n",
            "         [-3.0055e-02, -6.3231e-02,  1.8143e-01,  ..., -1.2302e-03,\n",
            "          -2.2691e-02, -7.4632e-02],\n",
            "         [-1.9963e-01, -1.0041e-02,  2.4487e-01,  ...,  1.4401e-01,\n",
            "          -8.6816e-02,  3.9923e-02]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim = 768, num_heads = 4, bias = False, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "\n",
        "        self.embed_dim   = embed_dim\n",
        "        self.num_heads   = num_heads\n",
        "        self.head_dim    = embed_dim // num_heads\n",
        "\n",
        "        self.query   = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.key     = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.value   = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "        self.out     = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, _ = x.size()\n",
        "\n",
        "        q = self.query(x).view(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        k = self.key(x).view(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = self.value(x).view(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # do NOT use causal attention as we are not dealing with sequential data (image patches are unordered)\n",
        "        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        out = (attn @ v).permute(0, 2, 1, 3).reshape(B, N, self.embed_dim)\n",
        "\n",
        "        out = self.out(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "TnZyK7QpeCOG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSA = SelfAttention()\n",
        "LN = nn.LayerNorm(embeddings.shape, bias=False)\n",
        "\n",
        "MSA(LN(embeddings))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX1RELy8elLi",
        "outputId": "0ea04dac-2a56-4bd7-a993-cea06b39dd90"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.2489,  0.1068,  0.2229,  ..., -0.2221, -0.5877,  0.2212],\n",
              "         [ 0.2648,  0.0926,  0.1939,  ..., -0.2076, -0.6067,  0.2941],\n",
              "         [ 0.2649,  0.0987,  0.2033,  ..., -0.2089, -0.6147,  0.2913],\n",
              "         ...,\n",
              "         [ 0.2570,  0.0990,  0.2022,  ..., -0.2134, -0.5974,  0.2629],\n",
              "         [ 0.2574,  0.0963,  0.2004,  ..., -0.2156, -0.5942,  0.2632],\n",
              "         [ 0.2552,  0.1006,  0.2089,  ..., -0.2170, -0.5923,  0.2500]],\n",
              "\n",
              "        [[ 0.2255,  0.1066,  0.2066,  ..., -0.2092, -0.5811,  0.2345],\n",
              "         [ 0.2371,  0.0945,  0.1928,  ..., -0.1960, -0.5817,  0.2740],\n",
              "         [ 0.2345,  0.0960,  0.1916,  ..., -0.2061, -0.5797,  0.2649],\n",
              "         ...,\n",
              "         [ 0.2299,  0.1054,  0.2050,  ..., -0.1972, -0.5866,  0.2527],\n",
              "         [ 0.2289,  0.1020,  0.1959,  ..., -0.1942, -0.5860,  0.2612],\n",
              "         [ 0.2379,  0.0993,  0.1998,  ..., -0.1937, -0.5869,  0.2707]],\n",
              "\n",
              "        [[ 0.2527,  0.1149,  0.2157,  ..., -0.2263, -0.5973,  0.2352],\n",
              "         [ 0.2642,  0.1064,  0.1812,  ..., -0.2145, -0.6192,  0.3105],\n",
              "         [ 0.2585,  0.0993,  0.1728,  ..., -0.2251, -0.5967,  0.2861],\n",
              "         ...,\n",
              "         [ 0.2626,  0.1080,  0.1848,  ..., -0.2032, -0.6185,  0.3118],\n",
              "         [ 0.2642,  0.1123,  0.1783,  ..., -0.2062, -0.6343,  0.3263],\n",
              "         [ 0.2540,  0.1084,  0.2002,  ..., -0.2133, -0.5991,  0.2667]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.2471,  0.1122,  0.2086,  ..., -0.2314, -0.5840,  0.2291],\n",
              "         [ 0.2617,  0.0976,  0.1794,  ..., -0.2177, -0.5997,  0.2987],\n",
              "         [ 0.2559,  0.0963,  0.1848,  ..., -0.2258, -0.5779,  0.2638],\n",
              "         ...,\n",
              "         [ 0.2637,  0.0993,  0.1845,  ..., -0.2174, -0.6021,  0.2956],\n",
              "         [ 0.2591,  0.1029,  0.1852,  ..., -0.2209, -0.5979,  0.2860],\n",
              "         [ 0.2569,  0.1044,  0.1841,  ..., -0.2244, -0.6034,  0.2868]],\n",
              "\n",
              "        [[ 0.2100,  0.0801,  0.1892,  ..., -0.1998, -0.5244,  0.1943],\n",
              "         [ 0.2427,  0.0673,  0.1562,  ..., -0.1740, -0.5821,  0.3389],\n",
              "         [ 0.2269,  0.0736,  0.1560,  ..., -0.1870, -0.5655,  0.2964],\n",
              "         ...,\n",
              "         [ 0.2089,  0.0761,  0.1748,  ..., -0.1978, -0.5208,  0.2048],\n",
              "         [ 0.2142,  0.0720,  0.1817,  ..., -0.1976, -0.5172,  0.2093],\n",
              "         [ 0.2114,  0.0782,  0.1827,  ..., -0.1973, -0.5225,  0.2036]],\n",
              "\n",
              "        [[ 0.2469,  0.1155,  0.2353,  ..., -0.2266, -0.5995,  0.2421],\n",
              "         [ 0.2577,  0.1085,  0.2079,  ..., -0.2158, -0.6178,  0.3032],\n",
              "         [ 0.2590,  0.1027,  0.1998,  ..., -0.2182, -0.6124,  0.3122],\n",
              "         ...,\n",
              "         [ 0.2505,  0.1120,  0.2289,  ..., -0.2244, -0.5942,  0.2511],\n",
              "         [ 0.2509,  0.1086,  0.2249,  ..., -0.2254, -0.5906,  0.2539],\n",
              "         [ 0.2529,  0.1122,  0.2205,  ..., -0.2226, -0.6067,  0.2737]]],\n",
              "       grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o3QFUhzvHGH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, embed_dim = 768, bias = False, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(embed_dim, embed_dim * 4, bias=bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(embed_dim * 4, embed_dim, bias=bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim = 768, bias = False):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(embed_dim, bias=bias)\n",
        "        self.attn = SelfAttention(embed_dim, bias=bias)\n",
        "        self.ln_2 = nn.LayerNorm(embed_dim, bias=bias)\n",
        "        self.mlp = MLP(embed_dim, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t3WPzOJGjb08"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim = 768, num_layers = 4, out_dim = 10, bias = False, dropout = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            pe = PatchEmbedding(),\n",
        "            drop = nn.Dropout(dropout),\n",
        "            h = nn.ModuleList([Block() for _ in range(num_layers)]),\n",
        "            ln_f = nn.LayerNorm(embed_dim)\n",
        "        ))\n",
        "        self.head = nn.Linear(embed_dim, out_dim, bias=False)\n",
        "\n",
        "vit = ViT()\n",
        "vit(torch.stack([train_data[i][0] for i in range(10)]))\n",
        "\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self):\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        return n_params\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.transformer.pe(x)\n",
        "        x = self.transformer.drop(emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        class_token = x[:, 0]\n",
        "        logits = self.head(class_token)\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "8B2o24X4jvZv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = ViT()\n",
        "vit(torch.stack([train_data[i][0] for i in range(10)]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ2FFcp-j5nR",
        "outputId": "c5c14e0c-c22d-425a-c6d8-3b1f3cfdbb9c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 28.42M\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.8224e-01, -3.7082e-01,  5.7170e-01, -1.7401e-01,  1.3356e-01,\n",
              "         -2.1121e-01, -1.3950e-01,  8.3914e-01, -3.8696e-01, -2.5666e-01],\n",
              "        [-3.3041e-01, -5.4182e-01,  5.2174e-01, -4.4653e-01,  1.6548e-01,\n",
              "         -1.6080e-02, -2.4076e-01,  9.8670e-01, -4.2810e-01, -5.0980e-01],\n",
              "        [-5.1813e-01, -3.1301e-01,  4.0145e-01, -2.6649e-02,  2.5983e-01,\n",
              "         -1.8473e-01, -3.2406e-01,  8.4892e-01, -4.7218e-01, -4.1467e-01],\n",
              "        [-4.1519e-01, -5.1449e-01,  6.7793e-01, -1.9332e-01,  1.5040e-01,\n",
              "         -2.7068e-04, -1.4009e-01,  8.9058e-01, -6.0834e-01, -4.3051e-01],\n",
              "        [-3.3792e-01, -6.3134e-01,  4.4530e-01, -3.9470e-02,  1.9257e-01,\n",
              "         -4.1653e-02, -2.3850e-01,  8.4040e-01, -5.9365e-01, -3.3698e-01],\n",
              "        [-6.3377e-01, -5.7146e-01,  5.5477e-01,  6.4145e-02,  7.1113e-02,\n",
              "          1.0503e-01, -1.1914e-01,  8.2762e-01, -4.7477e-01, -4.8431e-01],\n",
              "        [-5.1507e-01, -4.0855e-01,  4.6911e-01, -1.5588e-01,  2.6813e-01,\n",
              "         -2.8914e-02, -2.9787e-01,  7.6909e-01, -5.3660e-01, -2.6023e-01],\n",
              "        [-4.3397e-01, -3.1678e-01,  2.8451e-01,  2.1144e-02,  3.4217e-01,\n",
              "          7.6309e-03, -2.6986e-01,  8.6528e-01, -5.2605e-01, -3.1666e-01],\n",
              "        [-4.6012e-01, -4.4782e-01,  5.2296e-01,  2.3545e-01,  3.0305e-01,\n",
              "         -2.2094e-01, -3.9884e-01,  8.7333e-01, -5.9183e-01, -1.6128e-01],\n",
              "        [-4.9930e-01, -3.8292e-01,  4.9462e-01,  1.0316e-01,  2.8613e-01,\n",
              "          1.1561e-02, -3.6378e-01,  8.1694e-01, -3.6815e-01, -2.2169e-01]],\n",
              "       grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T3nsCM7PkCCJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}